# Data-Pipeline-Python

## Objective
The objective of this project to implement data-pipeline workflow of clientbook application. It is meant to read configurations, execute a workflow and store generated output.

The idea is to make it general workflow executor, so that it can support flexible design with multiple data-sources, generic runtime-environment for transformations, and flexible data aggregated dat storage model.

## Inputs
These are the expected inputs to this program. In this release, we are expecting to call it from existing data-pipeline job just before the execution of metric script. 

The data-pipeline will prepare query all the following sections before forking a new process to execute metric script,
* Initializing Global Variables
* Initializing Model Variables
* Initializing Datasources
* Initializing Metric Variables
* Initializing Metric-Datasource

The data-pipeline-python is implemented as an extension to data-pipeline, and expects following parameters from data-pipeline,
* metric_id
* model_id
* start_date
* end_date
* data_prepare_script - script generated by data-pipeline in base64-encoded format (easy to transmit without loss on network)